{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "df_raw = pd.read_csv(\"data/untappd_Beer_ratings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw['brewery'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df_raw['brewery'].sort_values().unique():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw['style'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df_raw['style'].sort_values().unique():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> lets clean the numerical data first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take out parenthese in rating\n",
    "df_raw['rating'] = df_raw['rating'].apply(lambda x: x.strip(')'))\n",
    "df_raw['rating'] = df_raw['rating'].apply(lambda x: x.strip('('))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to float\n",
    "df_raw['rating'] = df_raw['rating'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_raw['rating'].unique()\n",
    "# we see in the unique values that some are rounded to the thousandths place\n",
    "# we want to limit the scope of the data by rounding to the tenth place\n",
    "df_raw['rating']=df_raw['rating'].apply(lambda x: round(x,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw['rating'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets clean ibu\n",
    "df_raw['ibu'] = df_raw['ibu'].apply(lambda x: x.strip(' IBU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_raw['ibu'].unique()\n",
    "# only odd value is 'N/A'\n",
    "df_raw['ibu'] = df_raw['ibu'].replace('N/A', 0)\n",
    "df_raw['ibu'] = df_raw['ibu'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets clean abv\n",
    "df_raw['abv'] = df_raw['abv'].apply(lambda x: x.strip('% ABV'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_raw['abv'].unique()\n",
    "# there's a weird 'N/' value\n",
    "df_raw['abv'] = df_raw['abv'].replace('N/', 0)\n",
    "df_raw['abv'] = df_raw['abv'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw['abv'] = df_raw['abv'].apply(lambda x: round(x,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_raw['abv'].unique()\n",
    "df_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_raw[['abv', 'ibu', 'rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(kind='scatter', x='abv', y='rating', alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(kind='scatter', x='ibu', y='rating', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 12})\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_X = df.iloc[:,0:-1]\n",
    "df_train_y = df.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(df_train_X, df_train_y,test_size=0.3,random_state=3)\n",
    "print(len(X_test), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr = Ridge(alpha=0.01) # higher the alpha value, more restriction on the coefficients; low alpha > more generalization, coefficients are barely\n",
    "# restricted and in this case linear and ridge regression resembles\n",
    "rr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr100 = Ridge(alpha=100) #  comparison with alpha value\n",
    "rr100.fit(X_train, y_train)\n",
    "train_score=lr.score(X_train, y_train)\n",
    "test_score=lr.score(X_test, y_test)\n",
    "Ridge_train_score = rr.score(X_train,y_train)\n",
    "Ridge_test_score = rr.score(X_test, y_test)\n",
    "Ridge_train_score100 = rr100.score(X_train,y_train)\n",
    "Ridge_test_score100 = rr100.score(X_test, y_test)\n",
    "print(\"linear regression train score:\", train_score)\n",
    "print(\"linear regression test score:\", test_score)\n",
    "print(\"ridge regression train score low alpha:\", Ridge_train_score)\n",
    "print(\"ridge regression test score low alpha:\", Ridge_test_score)\n",
    "print(\"ridge regression train score high alpha:\", Ridge_train_score100)\n",
    "print(\"ridge regression test score high alpha:\", Ridge_test_score100)\n",
    "plt.plot(rr.coef_,alpha=0.7,linestyle='none',marker='*',markersize=5,color='red',label=r'Ridge; $\\alpha = 0.01$',zorder=7) # zorder for ordering the markers\n",
    "plt.plot(rr100.coef_,alpha=0.5,linestyle='none',marker='d',markersize=6,color='blue',label=r'Ridge; $\\alpha = 100$') # alpha here is for transparency\n",
    "# plt.plot(lr.coef_,alpha=0.4,linestyle='none',marker='o',markersize=7,color='green',label='Linear Regression')\n",
    "plt.xlabel('Coefficient Index',fontsize=16)\n",
    "plt.ylabel('Coefficient Magnitude',fontsize=16)\n",
    "plt.legend(fontsize=13,loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model we are using\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# Instantiate model with 1000 decision trees\n",
    "rf = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
    "# Train the model on training data\n",
    "#rf.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the forest's predict method on the test data\n",
    "predictions = rf.predict(X_test)\n",
    "# Calculate the absolute errors\n",
    "errors = abs(predictions - y_test)\n",
    "# Print out the mean absolute error (mae)\n",
    "print('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score=rf.score(X_train, y_train)\n",
    "test_score=rf.score(X_test, y_test)\n",
    "print(\"random forest train score:\", train_score)\n",
    "print(\"random forest test score:\", test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA SUCKS \n",
    "### lets clean it up some more, do some feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_style(x):\n",
    "    x = x.lower()\n",
    "    types = ['ale', 'stout', 'porter', 'sour', 'lager', 'pilsner', 'ipa', 'cider', 'wine', 'beer']\n",
    "    found = False\n",
    "    for a in types:\n",
    "        if a in x:\n",
    "            return a\n",
    "            found = True\n",
    "            \n",
    "    if found == False:\n",
    "        return 'other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_style('something asf awrg aeh pilsner asdf')\n",
    "# 'pilsner' in 'asdeh aer aerg a pilsner'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw['style'] = df_raw['style'].apply(lambda x: simple_style(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df_raw['style'].unique():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.head()\n",
    "df_raw['style'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.merge(df_raw, pd.get_dummies(df_raw['style']), left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.head()\n",
    "df_all.drop(['brewery', 'name', 'id', 'Unnamed: 0', 'style'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_X = df_all.drop('rating', axis=1)\n",
    "df_train_y = df_all.iloc[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(df_train_X, df_train_y,test_size=0.3,random_state=3)\n",
    "print(len(X_test), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the forest's predict method on the test data\n",
    "predictions = rf.predict(X_test)\n",
    "# Calculate the absolute errors\n",
    "errors = abs(predictions - y_test)\n",
    "# Print out the mean absolute error (mae)\n",
    "print('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score=rf.score(X_train, y_train)\n",
    "test_score=rf.score(X_test, y_test)\n",
    "print(\"random forest train score:\", train_score)\n",
    "print(\"random forest test score:\", test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "sample_leaf_options = [1,5,10,50,100,200,500]\n",
    "for leaf_size in sample_leaf_options :\n",
    "    model = RandomForestRegressor(n_estimators = 200, oob_score = True, n_jobs = -1,random_state =50,max_features = \"auto\", min_samples_leaf = leaf_size)\n",
    "    model.fit(X_train, y_train)\n",
    "    train_score=model.score(X_train, y_train)\n",
    "    test_score=model.score(X_test, y_test)\n",
    "    print(leaf_size)\n",
    "    print(\"random forest train score:\", train_score)\n",
    "    print(\"random forest test score:\", test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "n_estimators = [100,200, 500, 1000, 2000]\n",
    "for n_estimator in n_estimators :\n",
    "    model = RandomForestRegressor(n_estimators = n_estimator, oob_score = True, n_jobs = -1,random_state =50,max_features = \"auto\", min_samples_leaf = 10)\n",
    "    model.fit(X_train, y_train)\n",
    "    train_score=model.score(X_train, y_train)\n",
    "    test_score=model.score(X_test, y_test)\n",
    "    print(n_estimator)\n",
    "    print(\"random forest train score:\", train_score)\n",
    "    print(\"random forest test score:\", test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_beers = pd.read_csv(\"data/beers.csv\")\n",
    "df_beers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_beers.organic.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_breweries = pd.read_csv(\"data/untappd_breweries_ratings.csv\")\n",
    "df_breweries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_breweries['brewery'] = df_breweries['brewery'].apply(lambda x: x.split('/', 3)[-1])\n",
    "df_breweries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_breweries['raters'] = df_breweries['raters'].apply(lambda x: x.split(' ', 1)[0])\n",
    "df_breweries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_breweries['rating'] = df_breweries['rating'].apply(lambda x: x.strip(')'))\n",
    "df_breweries['rating'] = df_breweries['rating'].apply(lambda x: x.strip('('))\n",
    "df_breweries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_breweries['ibu'] = df_breweries['ibu'].apply(lambda x: x.strip(' IBU'))\n",
    "df_breweries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_breweries['abv'] = df_breweries['abv'].apply(lambda x: x.strip('% ABV'))\n",
    "df_breweries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_breweries['date'] = df_breweries['date'].apply(lambda x: x.strip('Added '))\n",
    "df_breweries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_breweries['date'] = pd.to_datetime(df_breweries.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_breweries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_breweries['ibu'] = df_breweries['ibu'].replace('N/A', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_breweries['abv'].unique()\n",
    "df_breweries['abv'] = df_breweries['abv'].replace('N/', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_breweries['rating'].unique()\n",
    "df_breweries['rating'] = df_breweries['rating'].replace('N/A', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_breweries['raters'].sort_values().unique()\n",
    "df_breweries['raters'] = df_breweries['raters'].apply(lambda x: x.replace(\",\", \"\"))\n",
    "df_breweries['raters'].sort_values().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_breweries.info()\n",
    "for col in ['abv', 'ibu', 'rating', 'raters']:\n",
    "    df_breweries[col] = df_breweries[col].astype(float)\n",
    "df_breweries.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_breweries['rating']=df_breweries['rating'].apply(lambda x: round(x,1))\n",
    "df_breweries['abv']=df_breweries['abv'].apply(lambda x: round(x,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "(datetime.datetime.today()-df_breweries.loc[1,'date']).days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_breweries['days_since'] = df_breweries['date'].apply(lambda x: (datetime.datetime.today()-x).days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_breweries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_breweries['raters_per_day'] = df_breweries['raters']/df_breweries['days_since']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_breweries.head()\n",
    "df_breweries['raters_per_day'] = df_breweries['raters_per_day'].apply(lambda x: round(x,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_breweries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_breweries_test = df_breweries.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_breweries_test['style'] = df_breweries_test['style'].apply(lambda x: simple_style(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_breweries_test = df_breweries_test[['name','style', 'abv', 'rating', 'raters', 'raters_per_day']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_test = pd.merge(df_breweries_test, pd.get_dummies(df_breweries_test['style']), left_index=True, right_index=True)\n",
    "df_all_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_test['raters_per_day'].unique()\n",
    "df_all_test['raters_per_day'] = df_all_test['raters_per_day'].replace('nan', 0)\n",
    "df_all_test['raters_per_day'] = df_all_test['raters_per_day'].replace(np.inf, 0)\n",
    "df_all_test['raters_per_day'] = df_all_test['raters_per_day'].replace(np.nan, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nodrop = df_all_test.copy()\n",
    "df_all_test.drop('style', axis=1, inplace=True)\n",
    "df_all_test.drop('raters', axis=1, inplace=True)\n",
    "df_all_test.drop('raters_per_day', axis=1, inplace=True)\n",
    "df_all_test.drop('name', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_X = df_all_test.drop('rating', axis=1)\n",
    "df_train_y = df_all_test['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(df_train_X, df_train_y,test_size=0.3,random_state=3)\n",
    "print(len(X_test), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the forest's predict method on the test data\n",
    "predictions = rf.predict(X_test)\n",
    "# Calculate the absolute errors\n",
    "errors = abs(predictions - y_test)\n",
    "# Print out the mean absolute error (mae)\n",
    "print('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score=rf.score(X_train, y_train)\n",
    "test_score=rf.score(X_test, y_test)\n",
    "print(\"random forest train score:\", train_score)\n",
    "print(\"random forest test score:\", test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Lets remove the ones missing a value in abv and rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_all_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_clean[df_clean['abv'] != 0]\n",
    "df_clean = df_clean[df_clean['rating'] != 0]\n",
    "print(df_clean.shape)\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_X = df_clean.drop('rating', axis=1)\n",
    "df_train_y = df_clean['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(df_train_X, df_train_y,test_size=0.3,random_state=3)\n",
    "print(len(X_test), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the forest's predict method on the test data\n",
    "predictions = rf.predict(X_test)\n",
    "# Calculate the absolute errors\n",
    "errors = abs(predictions - y_test)\n",
    "# Print out the mean absolute error (mae)\n",
    "print('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score=rf.score(X_train, y_train)\n",
    "test_score=rf.score(X_test, y_test)\n",
    "print(\"random forest train score:\", train_score)\n",
    "print(\"random forest test score:\", test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP TIME\n",
    "\n",
    "So these results aren't super, we should have enough data, but not enough features. Looking on the website, the only beefy-ish piece of data we can grab without doing a more intensive scrape is the description. We can use NLP to do some feature creation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import QK\n",
    "\n",
    "# functions to clean\n",
    "# QK.process_data(texts)\n",
    "# QK.stop_stem(texts)\n",
    "# QK.generate_words(texts)\n",
    "# QK.find_features(texts) # returns dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_texts_raw = pd.read_csv(\"data/untappd_beer_texts.csv\")\n",
    "df_texts_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_texts = df_texts_raw['text']\n",
    "df_texts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_texts = QK.process_data(df_texts)\n",
    "df_texts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_texts = QK.stop_stem((df_texts.apply(str)))\n",
    "df_texts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = QK.generate_words(df_texts)\n",
    "all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab most 500 most common words\n",
    "\n",
    "word_features = list(all_words.keys())[:500]\n",
    "word_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# featuresets = [(find_features(text), label) for (text, label) in df_texts] #msgs or test_messages\n",
    "test_features = [QK.find_features(text, word_features) for text in df_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nlp = pd.merge(pd.DataFrame(df_texts_raw['name']),pd.DataFrame(test_features), left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nlp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nlp = pd.merge(df_nlp, df_nodrop, left_on = 'name_x', right_on = 'name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "while (len([col for col in df_nlp.columns if df_nlp[col].dtype=='object']) > 0):\n",
    "        [df_nlp.drop(col, axis=1, inplace=True) for col in df_nlp.columns if df_nlp[col].dtype=='object']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nlp.drop('name_x', axis=1, inplace=True)\n",
    "df_nlp.drop('name_y', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols[296]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cols = list(df_nlp.columns)\n",
    "# for i, col in enumerate(cols):\n",
    "#     print(i, type(df_nlp[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[col for col in cols if df_nlp[col].dtype=='object']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nlp_run = df_nlp.drop('name', axis=1)\n",
    "df_nlp_run = df_nlp.drop('style_y', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(df_nlp_run.columns)\n",
    "[col for col in cols if df_nlp[col].dtype=='object']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nlp_run = df_nlp_run.drop('name', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nlp_run.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_X = df_nlp_run.drop('rating', axis=1)\n",
    "df_train_y = df_nlp_run['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(df_train_X, df_train_y,test_size=0.3,random_state=3)\n",
    "print(len(X_test), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare a range of parameters to test\n",
    "max_features = ['auto', 'log2', 'sqrt', 0.2, 0.5]\n",
    "n_estimators = [100,500,1000,5000]\n",
    "min_samples_leaf = [1,5,10,50,100,200,500]\n",
    "n_jobs = [1,-1]\n",
    "#create and fit a random forest model, testing each paramter above\n",
    "model = RandomForestRegressor(oob_score=True, max_features='auto', n_estimators=500, min_samples_leaf = 1, n_jobs=1) #We have chosen to just normalize the data by default, you could GridsearchCV this is you wanted\n",
    "grid = GridSearchCV(estimator=model, verbose=10, param_grid=dict(n_jobs=n_jobs))\n",
    "grid.fit(X_train, y_train)\n",
    "print(grid)\n",
    "# summarize the results of the grid search\n",
    "print(grid.best_score_)\n",
    "# print(grid.best_estimator_.max_features)\n",
    "# print(grid.best_estimator_.n_estimators)\n",
    "print(grid.best_estimator_.min_samples_leaf)\n",
    "print(grid.best_estimator_.n_jobs)\n",
    "print(grid.best_estimator_.solver)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(oob_score=True, max_features='auto', n_estimators=500, min_samples_leaf = 1, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the forest's predict method on the test data\n",
    "predictions = rf.predict(X_test)\n",
    "# Calculate the absolute errors\n",
    "errors = abs(predictions - y_test)\n",
    "# Print out the mean absolute error (mae)\n",
    "print('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score=rf.score(X_train, y_train)\n",
    "test_score=rf.score(X_test, y_test)\n",
    "print(\"random forest train score:\", train_score)\n",
    "print(\"random forest test score:\", test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "dump(rf, 'rf_optimal.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST DIFFERENT MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 12})\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "rr = Ridge(alpha=0.01) # higher the alpha value, more restriction on the coefficients; low alpha > more generalization, coefficients are barely\n",
    "# restricted and in this case linear and ridge regression resembles\n",
    "rr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr100 = Ridge(alpha=100) #  comparison with alpha value\n",
    "rr100.fit(X_train, y_train)\n",
    "train_score=lr.score(X_train, y_train)\n",
    "test_score=lr.score(X_test, y_test)\n",
    "Ridge_train_score = rr.score(X_train,y_train)\n",
    "Ridge_test_score = rr.score(X_test, y_test)\n",
    "Ridge_train_score100 = rr100.score(X_train,y_train)\n",
    "Ridge_test_score100 = rr100.score(X_test, y_test)\n",
    "print(\"linear regression train score:\", train_score)\n",
    "print(\"linear regression test score:\", test_score)\n",
    "print(\"ridge regression train score low alpha:\", Ridge_train_score)\n",
    "print(\"ridge regression test score low alpha:\", Ridge_test_score)\n",
    "print(\"ridge regression train score high alpha:\", Ridge_train_score100)\n",
    "print(\"ridge regression test score high alpha:\", Ridge_test_score100)\n",
    "plt.plot(rr.coef_,alpha=0.7,linestyle='none',marker='*',markersize=5,color='red',label=r'Ridge; $\\alpha = 0.01$',zorder=7) # zorder for ordering the markers\n",
    "plt.plot(rr100.coef_,alpha=0.5,linestyle='none',marker='d',markersize=6,color='blue',label=r'Ridge; $\\alpha = 100$') # alpha here is for transparency\n",
    "# plt.plot(lr.coef_,alpha=0.4,linestyle='none',marker='o',markersize=7,color='green',label='Linear Regression')\n",
    "plt.xlabel('Coefficient Index',fontsize=16)\n",
    "plt.ylabel('Coefficient Magnitude',fontsize=16)\n",
    "plt.legend(fontsize=13,loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso()\n",
    "lasso.fit(X_train,y_train)\n",
    "train_score=lasso.score(X_train,y_train)\n",
    "test_score=lasso.score(X_test,y_test)\n",
    "coeff_used = np.sum(lasso.coef_!=0)\n",
    "print(\"training score:\", train_score )\n",
    "print(\"test score: \", test_score)\n",
    "print(\"number of features used: \", coeff_used)\n",
    "lasso001 = Lasso(alpha=0.01, max_iter=10e5)\n",
    "lasso001.fit(X_train,y_train)\n",
    "train_score001=lasso001.score(X_train,y_train)\n",
    "test_score001=lasso001.score(X_test,y_test)\n",
    "coeff_used001 = np.sum(lasso001.coef_!=0)\n",
    "print(\"training score for alpha=0.01:\", train_score001) \n",
    "print(\"test score for alpha =0.01: \", test_score001)\n",
    "print(\"number of features used: for alpha =0.01:\", coeff_used001)\n",
    "lasso00001 = Lasso(alpha=0.0001, max_iter=10e5)\n",
    "lasso00001.fit(X_train,y_train)\n",
    "train_score00001=lasso00001.score(X_train,y_train)\n",
    "test_score00001=lasso00001.score(X_test,y_test)\n",
    "coeff_used00001 = np.sum(lasso00001.coef_!=0)\n",
    "print(\"training score for alpha=0.0001:\", train_score00001) \n",
    "print(\"test score for alpha =0.0001: \", test_score00001)\n",
    "print(\"number of features used: for alpha =0.0001:\", coeff_used00001)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train,y_train)\n",
    "lr_train_score=lr.score(X_train,y_train)\n",
    "lr_test_score=lr.score(X_test,y_test)\n",
    "print(\"LR training score:\", lr_train_score) \n",
    "print(\"LR test score: \", lr_test_score)\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(lasso.coef_,alpha=0.7,linestyle='none',marker='*',markersize=5,color='red',label=r'Lasso; $\\alpha = 1$',zorder=7) # alpha here is for transparency\n",
    "plt.plot(lasso001.coef_,alpha=0.5,linestyle='none',marker='d',markersize=6,color='blue',label=r'Lasso; $\\alpha = 0.01$') # alpha here is for transparency\n",
    "\n",
    "plt.xlabel('Coefficient Index',fontsize=16)\n",
    "plt.ylabel('Coefficient Magnitude',fontsize=16)\n",
    "plt.legend(fontsize=13,loc=4)\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(lasso.coef_,alpha=0.7,linestyle='none',marker='*',markersize=5,color='red',label=r'Lasso; $\\alpha = 1$',zorder=7) # alpha here is for transparency\n",
    "plt.plot(lasso001.coef_,alpha=0.5,linestyle='none',marker='d',markersize=6,color='blue',label=r'Lasso; $\\alpha = 0.01$') # alpha here is for transparency\n",
    "plt.plot(lasso00001.coef_,alpha=0.8,linestyle='none',marker='v',markersize=6,color='black',label=r'Lasso; $\\alpha = 0.00001$') # alpha here is for transparency\n",
    "plt.plot(lr.coef_,alpha=0.7,linestyle='none',marker='o',markersize=5,color='green',label='Linear Regression',zorder=2)\n",
    "plt.xlabel('Coefficient Index',fontsize=16)\n",
    "plt.ylabel('Coefficient Magnitude',fontsize=16)\n",
    "plt.legend(fontsize=13,loc=4)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "EN = ElasticNet()\n",
    "EN.fit(X_train,y_train)\n",
    "train_score = EN.score(X_train,y_train)\n",
    "test_score = EN.score(X_test,y_test)\n",
    "coeff_used = np.sum(EN.coef_!=0)\n",
    "print(\"training score:\", train_score )\n",
    "print(\"test score: \", test_score)\n",
    "print(\"number of features used: \", coeff_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "svr = SVR(gamma='scale', C=1.0, epsilon=0.2)\n",
    " \n",
    "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.2, gamma='scale',\n",
    "    kernel='linear', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
    "\n",
    "svr.fit(X_train,y_train)\n",
    "train_score = svr.score(X_train,y_train)\n",
    "test_score = svr.score(X_test,y_test)\n",
    "coeff_used = np.sum(EN.coef_!=0)\n",
    "print(\"training score:\", train_score )\n",
    "print(\"test score: \", test_score)\n",
    "print(\"number of features used: \", coeff_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV # Grid Search for tuning the Ridge Regression\n",
    "\n",
    "#prepare a range of parameters to test\n",
    "alphas = np.array([1,0.1,0.01,0.001,0.0001,0])\n",
    "fit_interceptOptions = ([True, False])\n",
    "solverOptions = (['svd', 'cholesky', 'sparse_cg', 'sag'])\n",
    "#create and fit a ridge regression model, testing each alpha\n",
    "model = Ridge(normalize=True) #We have chosen to just normalize the data by default, you could GridsearchCV this is you wanted\n",
    "grid = GridSearchCV(estimator=model, param_grid=dict(alpha=alphas, fit_intercept=fit_interceptOptions, solver=solverOptions))\n",
    "grid.fit(X_train, y_train)\n",
    "print(grid)\n",
    "# summarize the results of the grid search\n",
    "print(grid.best_score_)\n",
    "print(grid.best_estimator_.alpha)\n",
    "print(grid.best_estimator_.fit_intercept)\n",
    "print(grid.best_estimator_.solver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"data/test_data.csv\")\n",
    "test.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
